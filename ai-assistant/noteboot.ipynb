{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Assistant Prototype with Conversation Memory & Session IDs\n",
    "\n",
    "This notebook demonstrates an updated chatbot assistant that:\n",
    "- Uses **LangChain** with an OpenAI LLM and a Pinecone‑hosted RAG knowledge base.\n",
    "- Integrates a **weather tool** (using wttr.in as a demo API).\n",
    "- Maintains **conversation memory** using session IDs so that previous Q&A pairs are considered.\n",
    "- Supports both **non‑streaming** and **streaming** output.\n",
    "\n",
    "Make sure to set the following environment variables (e.g., in your shell or within the notebook):\n",
    "- `OPENAI_API_KEY`\n",
    "- `PINECONE_API_KEY`\n",
    "- `PINECONE_ENVIRONMENT`\n",
    "- `PINECONE_INDEX_NAME`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages.\n",
    "# Run this cell to ensure you have all dependencies installed.\n",
    "!pip install langchain langchain_community pinecone-client openai requests nest_asyncio sse-starlette fastapi uvicorn tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import asyncio\n",
    "import requests\n",
    "\n",
    "import pinecone\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone as LC_Pinecone\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# For streaming via Server-Sent Events (SSE)\n",
    "from sse_starlette.sse import EventSourceResponse\n",
    "\n",
    "# Allow nested event loops in notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global in-memory conversation memory.\n",
    "# Maps a session_id (string) to a list of conversation turns.\n",
    "chat_memories = {}\n",
    "\n",
    "def get_current_weather(location: str, unit: str = \"Celsius\") -> str:\n",
    "    \"\"\"\n",
    "    Fetch current weather information for a given location.\n",
    "    (Uses wttr.in as a demo API in place of Tavily.)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"https://wttr.in/{location}?format=3\")\n",
    "        if response.status_code == 200:\n",
    "            return response.text.strip()\n",
    "        else:\n",
    "            return f\"Could not fetch weather data for {location}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching weather data: {str(e)}\"\n",
    "\n",
    "# Wrap the weather function as a LangChain Tool.\n",
    "weather_tool = Tool(\n",
    "    name=\"get_current_weather\",\n",
    "    func=get_current_weather,\n",
    "    description=(\n",
    "        \"Useful for when you need to get the current weather information for a location. \"\n",
    "        \"The input to this tool should be a location name (e.g., 'Paris').\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotAssistant:\n",
    "    def __init__(self, streaming: bool = False):\n",
    "        self.streaming = streaming\n",
    "        callbacks = [StreamingStdOutCallbackHandler()] if streaming else []\n",
    "        \n",
    "        # Initialize the LLM (here, GPT‑4 via ChatOpenAI).\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4\",\n",
    "            streaming=streaming,\n",
    "            callbacks=callbacks,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        \n",
    "        # Set up OpenAI embeddings.\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "        # Initialize Pinecone using environment variables.\n",
    "        pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "        pinecone_env = os.environ.get(\"PINECONE_ENVIRONMENT\", \"us-east-1\")\n",
    "        pinecone_index_name = os.environ.get(\"PINECONE_INDEX_NAME\", \"schh\")\n",
    "        if not pinecone_api_key or not pinecone_env or not pinecone_index_name:\n",
    "            raise Exception(\"Missing one or more required Pinecone environment variables.\")\n",
    "        # pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)\n",
    "\n",
    "        # Connect to the existing Pinecone index.\n",
    "        self.vectorstore = LC_Pinecone.from_existing_index(\n",
    "            index_name=pinecone_index_name,\n",
    "            embedding=self.embeddings,\n",
    "            text_key=\"text\"  # Assumes documents are indexed under the \"text\" field.\n",
    "        )\n",
    "        \n",
    "        # Build a RetrievalQA chain that uses the Pinecone vectorstore.\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",  # \"stuff\" simply concatenates documents.\n",
    "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "        \n",
    "        # Initialize an agent that has access to the weather tool.\n",
    "        self.agent = initialize_agent(\n",
    "            tools=[weather_tool],\n",
    "            llm=self.llm,\n",
    "            agent=\"zero-shot-react-description\",\n",
    "            verbose=True,\n",
    "        )\n",
    "        \n",
    "    def answer_query(self, query: str, session_id: Optional[str] = None) -> dict:\n",
    "        \"\"\"\n",
    "        Answer the given query while considering previous conversation turns if provided.\n",
    "        If a session_id is provided and history exists, that history is prepended to the query.\n",
    "        Weather-related queries are handled by the agent (with the weather tool);\n",
    "        other queries use the RetrievalQA chain.\n",
    "        \n",
    "        After obtaining an answer, the new Q/A pair is appended to the conversation history.\n",
    "        \n",
    "        Returns:\n",
    "            dict: { \"answer\": <text>, \"sources\": [<source1>, ...] }\n",
    "        \"\"\"\n",
    "        original_query = query  # Preserve the original question.\n",
    "        if session_id is not None:\n",
    "            history = chat_memories.get(session_id, [])\n",
    "            if history:\n",
    "                context = \"\\n\".join(history)\n",
    "                query = f\"Conversation History:\\n{context}\\n\\nNew Question: {query}\"\n",
    "        \n",
    "        if \"weather\" in original_query.lower():\n",
    "            answer = self.agent.run(query)\n",
    "            result = {\"answer\": answer, \"sources\": []}\n",
    "        else:\n",
    "            chain_result = self.qa_chain(query)\n",
    "            answer = chain_result.get(\"result\", \"\")\n",
    "            sources = []\n",
    "            for doc in chain_result.get(\"source_documents\", []):\n",
    "                source_info = doc.metadata.get(\"source\", \"Unknown source\")\n",
    "                sources.append(source_info)\n",
    "            result = {\"answer\": answer, \"sources\": sources}\n",
    "        \n",
    "        # Update conversation memory.\n",
    "        if session_id is not None:\n",
    "            memory_entry = f\"Q: {original_query}\\nA: {answer}\"\n",
    "            if session_id in chat_memories:\n",
    "                chat_memories[session_id].append(memory_entry)\n",
    "            else:\n",
    "                chat_memories[session_id] = [memory_entry]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "# Instantiate the assistant (defaulting to non‑streaming mode).\n",
    "assistant = ChatbotAssistant(streaming=False)\n",
    "\n",
    "# Define request/response models.\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    session_id: Optional[str] = None  # Client may supply a session ID.\n",
    "    streaming: Optional[bool] = False  # Option for streaming output.\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    answer: str\n",
    "    sources: List[str] = []\n",
    "    session_id: str  # Returned so the client can continue the conversation.\n",
    "\n",
    "@app.post(\"/chat\", response_model=QueryResponse)\n",
    "async def chat_endpoint(request: QueryRequest):\n",
    "    \"\"\"\n",
    "    Synchronous (non‑streaming) endpoint.\n",
    "    If no session_id is provided, a new one is generated.\n",
    "    \"\"\"\n",
    "    global assistant\n",
    "    if request.streaming != assistant.streaming:\n",
    "        assistant = ChatbotAssistant(streaming=request.streaming)\n",
    "    session_id = request.session_id or str(uuid.uuid4())\n",
    "    result = assistant.answer_query(request.query, session_id=session_id)\n",
    "    return QueryResponse(answer=result[\"answer\"], sources=result[\"sources\"], session_id=session_id)\n",
    "\n",
    "# Custom callback handler for streaming tokens.\n",
    "class QueueCallbackHandler(StreamingStdOutCallbackHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.queue = asyncio.Queue()\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        asyncio.create_task(self.queue.put(token))\n",
    "\n",
    "async def stream_tokens(query: str, session_id: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Generator that streams tokens as SSE events.\n",
    "    If conversation history exists for the session, it is prepended.\n",
    "    \"\"\"\n",
    "    original_query = query\n",
    "    if session_id is not None:\n",
    "        history = chat_memories.get(session_id, [])\n",
    "        if history:\n",
    "            context = \"\\n\".join(history)\n",
    "            query = f\"Conversation History:\\n{context}\\n\\nNew Question: {query}\"\n",
    "    handler = QueueCallbackHandler()\n",
    "    llm_stream = ChatOpenAI(\n",
    "        model_name=\"gpt-4\",\n",
    "        streaming=True,\n",
    "        callbacks=[handler],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    if \"weather\" in original_query.lower():\n",
    "        agent = initialize_agent(\n",
    "            tools=[weather_tool],\n",
    "            llm=llm_stream,\n",
    "            agent=\"zero-shot-react-description\",\n",
    "            verbose=True,\n",
    "        )\n",
    "        loop = asyncio.get_event_loop()\n",
    "        future = loop.run_in_executor(None, agent.run, query)\n",
    "    else:\n",
    "        retrieval_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm_stream,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=assistant.vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "        loop = asyncio.get_event_loop()\n",
    "        future = loop.run_in_executor(None, retrieval_chain, query)\n",
    "    \n",
    "    answer_parts = []\n",
    "    while True:\n",
    "        try:\n",
    "            token = await asyncio.wait_for(handler.queue.get(), timeout=1.0)\n",
    "            answer_parts.append(token)\n",
    "            yield f\"data: {token}\\n\\n\"\n",
    "        except asyncio.TimeoutError:\n",
    "            if future.done():\n",
    "                break\n",
    "    answer = \"\".join(answer_parts)\n",
    "    if session_id is not None:\n",
    "        memory_entry = f\"Q: {original_query}\\nA: {answer}\"\n",
    "        if session_id in chat_memories:\n",
    "            chat_memories[session_id].append(memory_entry)\n",
    "        else:\n",
    "            chat_memories[session_id] = [memory_entry]\n",
    "    yield \"data: [DONE]\\n\\n\"\n",
    "\n",
    "@app.post(\"/chat/stream\")\n",
    "async def chat_stream_endpoint(request: QueryRequest):\n",
    "    \"\"\"\n",
    "    Streaming endpoint (SSE) that uses a session ID to track conversation history.\n",
    "    \"\"\"\n",
    "    session_id = request.session_id or str(uuid.uuid4())\n",
    "    return EventSourceResponse(stream_tokens(request.query, session_id=session_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: ebe466dd-dcff-4573-8e7c-540e3970b10c\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to use the get_current_weather tool to find out the current weather in London.\n",
      "Action: get_current_weather\n",
      "Action Input: London\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLondon: ☀️   +1°C\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The weather in London is sunny with a temperature of +1°C.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Query 1: What's the weather in London?\n",
      "Answer 1: The weather in London is sunny with a temperature of +1°C.\n",
      "Sources 1: []\n",
      "\n",
      "Query 2: And what about tomorrow?\n",
      "Answer 2: I'm sorry, but I don't have the ability to provide weather forecasts.\n",
      "Sources 2: ['./sports/golf/2024GolfTownHall2.pdf', 'clubs/computer-club/education-weekly.md', 'clubs/computer-club/education-weekly.md', 'clubs/computer-club/education-weekly.md']\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a non‑streaming assistant.\n",
    "assistant = ChatbotAssistant(streaming=False)\n",
    "\n",
    "# Create a new session.\n",
    "session_id = str(uuid.uuid4())\n",
    "print(\"Session ID:\", session_id)\n",
    "\n",
    "# Example 1: Weather query.\n",
    "query1 = \"What's the weather in London?\"\n",
    "result1 = assistant.answer_query(query1, session_id=session_id)\n",
    "print(\"Query 1:\", query1)\n",
    "print(\"Answer 1:\", result1[\"answer\"])\n",
    "print(\"Sources 1:\", result1[\"sources\"])\n",
    "\n",
    "# Example 2: Follow-up query (using conversation history).\n",
    "query2 = \"And what about tomorrow?\"\n",
    "result2 = assistant.answer_query(query2, session_id=session_id)\n",
    "print(\"\\nQuery 2:\", query2)\n",
    "print(\"Answer 2:\", result2[\"answer\"])\n",
    "print(\"Sources 2:\", result2[\"sources\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Session ID: 4a684c4b-f013-4800-86a4-50c99685ce18\n",
      "Streaming Query: Tell me modification requests.\n",
      "data: [DONE]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def test_streaming():\n",
    "    session_id = str(uuid.uuid4())\n",
    "    print(\"Streaming Session ID:\", session_id)\n",
    "    query = \"Tell me about the benefits of renewable energy.\"\n",
    "    print(\"Streaming Query:\", query)\n",
    "    async for token in stream_tokens(query, session_id=session_id):\n",
    "        print(token, end='')\n",
    "\n",
    "# Uncomment and run the cell below to test streaming.\n",
    "await test_streaming()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
